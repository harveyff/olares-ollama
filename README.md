# Olares-Ollama Proxy Server

A Go-based proxy server that runs in front of Ollama, providing unified model management and API forwarding capabilities.

## Features

- ğŸš€ **Automatic Model Download**: Automatically checks and downloads specified models on startup
- ğŸ“Š **Real-time Progress Monitoring**: Provides a web interface to view model download progress in real-time
- ğŸ”„ **API Proxy Forwarding**: Intelligently forwards API requests to Ollama server
- ğŸ¯ **Model Parameter Replacement**: Automatically replaces model parameters in requests with the configured model
- ğŸ›¡ï¸ **Interface Filtering**: Only exposes necessary API interfaces for improved security

## Quick Start

### Requirements

- Go 1.21+
- Ollama server running locally or remotely

### Installation and Usage

1. Clone the repository and enter the directory:
```bash
git clone <repository-url>
cd olares-ollama
```

2. Install dependencies:
```bash
go mod tidy
```

3. Set environment variables:
```bash
export OLLAMA_MODEL=llama2              # Model name to use
export OLLAMA_URL=http://localhost:11434  # Ollama server address
export PORT=8080                        # Proxy server port
export DOWNLOAD_TIMEOUT=60              # Download timeout in minutes
```

4. Run the server:
```bash
go run main.go
```

5. Access the web interface to view download progress:
```
http://localhost:8080
```

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_MODEL` | `llama2` | Target model name |
| `OLLAMA_URL` | `http://localhost:11434` | Ollama server address |
| `PORT` | `8080` | Proxy server port |
| `DOWNLOAD_TIMEOUT` | `60` | Model download timeout in minutes |

## API Interfaces

### Supported Interfaces

#### Model Management
- `GET /api/tags` - Get available model list (only returns the configured model)

#### Inference Interfaces (automatic model parameter replacement)
- `POST /api/generate` - Text generation
- `POST /api/chat` - Chat conversation
- `POST /api/embeddings` - Text embeddings

#### System Management (direct proxy)
- `GET /api/version` - Get version information
- `GET /api/ps` - Get running processes
- `POST /api/stop` - Stop model

#### Other
- `GET /health` - Health check
- `GET /api/progress` - Progress monitoring

### Usage Examples

```bash
# Chat request (model parameter will be automatically replaced)
curl -X POST http://localhost:8080/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "any-model-name",
    "messages": [
      {
        "role": "user", 
        "content": "Hello!"
      }
    ]
  }'

# Text generation
curl -X POST http://localhost:8080/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "any-model-name",
    "prompt": "Tell me a story",
    "stream": false
  }'
```

## Project Structure

```
olares-ollama/
â”œâ”€â”€ main.go                    # Program entry point
â”œâ”€â”€ go.mod                     # Go module file
â”œâ”€â”€ .gitignore                 # Git ignore file
â”œâ”€â”€ data/                      # Model data directory (local storage)
â”‚   â””â”€â”€ .gitkeep              # Keep directory structure
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.go          # Configuration management
â”‚   â”œâ”€â”€ ollama/
â”‚   â”‚   â””â”€â”€ client.go          # Ollama client
â”‚   â”œâ”€â”€ download/
â”‚   â”‚   â””â”€â”€ progress.go        # Download progress management
â”‚   â””â”€â”€ server/
â”‚       â”œâ”€â”€ server.go          # HTTP server
â”‚       â””â”€â”€ handlers.go        # Request handlers
â”œâ”€â”€ web/
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ index.html         # Frontend interface
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ API.md                 # API documentation
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ start.sh              # Startup script
â”œâ”€â”€ Dockerfile                 # Docker configuration
â”œâ”€â”€ docker-compose.yml         # Docker Compose configuration
â””â”€â”€ Makefile                  # Build script
```

## Development

### Build

```bash
go build -o olares-ollama main.go
```

### Run Tests

```bash
go test ./...
```

## Data Storage

### Local Mode
When running `docker-compose up`, model files will be stored in the project root's `data/` folder:
- `data/` - Ollama model data directory
- This directory is added to `.gitignore` and won't be committed to version control
- Deleting the `data/` directory will clean up all downloaded models

### Data Persistence
- When using Docker Compose, model data persists in the local `data/` directory
- Restarting containers won't lose downloaded models
- You can backup models by backing up the `data/` directory

## Notes

1. The first startup will automatically download the specified model, which may take a long time
2. Ensure the Ollama server is running and accessible
3. Model download progress can be viewed in real-time through the web interface
4. All inference request model parameters will be replaced with the configured model
5. Model files are stored in the `data/` directory and can be cleaned or backed up as needed

## Troubleshooting

### Model Download Timeout
If you encounter "context deadline exceeded" errors:

1. **Increase download timeout**:
   ```bash
   export DOWNLOAD_TIMEOUT=180  # Set to 3 hours
   ```

2. **Choose a smaller model**:
   ```bash
   export OLLAMA_MODEL=qwen3:0.6b  # Smaller model, faster download
   ```

3. **Check network connection**:
   - Ensure stable network connection
   - Check for firewall or proxy restrictions

4. **Manually download model**:
   ```bash
   # Manually download on Ollama server
   ollama pull qwen3:0.6b
   ```

### Common Issues

**Q: Download is slow?**
A: Choose smaller models like `qwen3:0.6b` or `phi3:mini`

**Q: How to clean downloaded models?**
A: Delete the `data/` directory: `rm -rf data/`

**Q: How to view download progress?**
A: Visit `http://localhost:8080` to see the web interface

**Q: Server startup failed?**
A: Check if Ollama service is running: `curl http://localhost:11434/api/version`

## License

MIT License